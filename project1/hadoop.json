{"#### Q1. Partitioner controls the partitioning of what data?\n": ["- [ ] final keys\n", "- [ ] final values\n", "- [x] intermediate keys\n", "- [ ] intermediate values\n"], "#### Q2. SQL Windowing functions are implemented in Hive using which keywords?\n": ["- [ ] UNION DISTINCT, RANK\n", "- [x] OVER, RANK\n", "- [ ] OVER, EXCEPT\n", "- [ ] UNION DISTINCT, RANK\n"], "#### Q3. Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?\n": ["- [ ] Add a partitioned shuffle to the Map job.\n", "- [x] Add a partitioned shuffle to the Reduce job.\n", "- [ ] Break the Reduce job into multiple, chained Reduce jobs.\n", "- [ ] Break the Reduce job into multiple, chained Map jobs.\n"], "#### Q4. Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?\n": ["- [ ] encrypted HTTP\n", "- [ ] unsigned HTTP\n", "- [ ] compressed HTTP\n", "- [x] signed HTTP\n"], "#### Q5. MapReduce jobs can be written in which language?\n": ["- [x] Java or Python\n", "- [ ] SQL only\n", "- [ ] SQL or Java\n", "- [ ] Python or SQL\n"], "#### Q6. To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?\n": ["- [ ] Reducer\n", "- [x] Combiner\n", "- [ ] Mapper\n", "- [ ] Counter\n"], "#### Q7. To verify job status, look for the value `___` in the `___`.\n": ["- [ ] SUCCEEDED; syslog\n", "- [x] SUCCEEDED; stdout\n", "- [ ] DONE; syslog\n", "- [ ] DONE; stdout\n"], "#### Q8. Which line of code implements a Reducer method in MapReduce 2.0?\n": ["- [x] public void reduce(Text key, Iterator<IntWritable> values, Context context){\u2026}\n", "- [ ] public static void reduce(Text key, IntWritable[] values, Context context){\u2026}\n", "- [ ] public static void reduce(Text key, Iterator<IntWritable> values, Context context){\u2026}\n", "- [ ] public void reduce(Text key, IntWritable[] values, Context context){\u2026}\n"], "#### Q9. To get the total number of mapped input records in a map job task, you should review the value of which counter?\n": ["- [ ] FileInputFormatCounter\n", "- [ ] FileSystemCounter\n", "- [ ] JobCounter\n", "- [x] TaskCounter (NOT SURE)\n"], "#### Q10. Hadoop Core supports which CAP capabilities?\n": ["- [x] A, P\n", "- [ ] C, A\n", "- [ ] C, P\n", "- [ ] C, A, P\n"], "#### Q11. What are the primary phases of a Reducer?\n": ["- [ ] combine, map, and reduce\n", "- [x] shuffle, sort, and reduce\n", "- [ ] reduce, sort, and combine\n", "- [ ] map, sort, and combine\n"], "#### Q12. To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the `___` service, which is `___`.\n": ["- [ ] Oozie; open source\n", "- [ ] Oozie; commercial software\n", "- [ ] Zookeeper; commercial software\n", "- [x] Zookeeper; open source\n"], "#### Q13. For high availability, use multiple nodes of which type?\n": ["- [ ] data\n", "- [x] name\n", "- [ ] memory\n", "- [ ] worker\n"], "#### Q14. DataNode supports which type of drives?\n": ["- [x] hot swappable\n", "- [ ] cold swappable\n", "- [ ] warm swappable\n", "- [ ] non-swappable\n"], "#### Q15. Which method is used to implement Spark jobs?\n": ["- [ ] on disk of all workers\n", "- [ ] on disk of the master node\n", "- [ ] in memory of the master node\n", "- [x] in memory of all workers\n"], "#### Q16. In a MapReduce job, where does the map() function run?\n": ["- [ ] on the reducer nodes of the cluster\n", "- [x] on the data nodes of the cluster (NOT SURE)\n", "- [ ] on the master node of the cluster\n", "- [ ] on every node of the cluster\n"], "#### Q17. To reference a master file for lookups during Mapping, what type of cache should be used?\n": ["- [x] distributed cache\n", "- [ ] local cache\n", "- [ ] partitioned cache\n", "- [ ] cluster cache\n"], "#### Q18. Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?\n": ["- [ ] cache inputs\n", "- [ ] reducer inputs\n", "- [ ] intermediate values\n", "- [x] map inputs\n"], "#### Q19. Which command imports data to Hadoop from a MySQL database?\n": ["- [ ] spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark\n", "- [ ] sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop\n", "- [x] sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop\n", "- [ ] spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark\n"], "#### Q20. In what form is Reducer output presented?\n": ["- [x] compressed (NOT SURE)\n", "- [ ] sorted\n", "- [ ] not sorted\n", "- [ ] encrypted\n"], "#### Q21. Which library should be used to unit test MapReduce code?\n": ["- [ ] JUnit\n", "- [ ] XUnit\n", "- [x] MRUnit\n", "- [ ] HadoopUnit\n"], "#### Q22. If you started the NameNode, then which kind of user must you be?\n": ["- [ ] hadoop-user\n", "- [x] super-user\n", "- [ ] node-user\n", "- [ ] admin-user\n"], "#### Q23. State \\_ between the JVMs in a MapReduce job\n": ["- [ ] can be configured to be shared\n", "- [ ] is partially shared\n", "- [ ] is shared\n", "- [x] is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)\n"], "#### Q24. To create a MapReduce job, what should be coded first?\n": ["- [ ] a static job() method\n", "- [x] a Job class and instance (NOT SURE)\n", "- [ ] a job() method\n", "- [ ] a static Job class\n"], "#### Q25. To connect Hadoop to AWS S3, which client should you use?\n": ["- [x] S3A\n", "- [ ] S3N\n", "- [ ] S3\n", "- [ ] the EMR S3\n"], "#### Q26. HBase works with which type of schema enforcement?\n": ["- [ ] schema on write\n", "- [ ] no schema\n", "- [ ] external schema\n", "- [x] schema on read\n"], "#### Q27. HDFS file are of what type?\n": ["- [ ] read-write\n", "- [ ] read-only\n", "- [ ] write-only\n", "- [x] append-only\n"], "#### Q28. A distributed cache file path can originate from what location?\n": ["- [ ] hdfs or top\n", "- [ ] http\n", "- [x] hdfs or http\n", "- [ ] hdfs\n"], "#### Q29. Which library should you use to perform ETL-type MapReduce jobs?\n": ["- [ ] Hive\n", "- [x] Pig\n", "- [ ] Impala\n", "- [ ] Mahout\n"], "#### Q30. What is the output of the Reducer?\n": ["- [ ] a relational table\n", "- [ ] an update to the input file\n", "- [ ] a single, combined list\n", "- [x] a set of <key, value> pairs\n","`map function processes a certain key-value pair and emits a certain number of key-value pairs and the Reduce function processes values grouped by the same key and emits another set of key-value pairs as output.`\n"], "#### Q31. To optimize a Mapper, what should you perform first?\n": ["- [ ] Override the default Partitioner.\n", "- [ ] Skip bad records.\n", "- [ ] Break up Mappers that do more than one task into multiple Mappers.\n", "- [ ] Combine Mappers that do one task into large Mappers.\n"], "#### Q32. When implemented on a public cloud, with what does Hadoop processing interact?\n": ["- [x] files in object storage\n", "- [ ] graph data in graph databases\n", "- [ ] relational data in managed RDBMS systems\n", "- [ ] JSON data in NoSQL databases\n"], "#### Q33. In the Hadoop system, what administrative mode is used for maintenance?\n": ["- [ ] data mode\n", "- [x] safe mode\n", "- [ ] single-user mode\n", "- [ ] pseudo-distributed mode\n"], "#### Q34. In what format does RecordWriter write an output file?\n": ["- [x] <key, value> pairs\n", "- [ ] keys\n", "- [ ] values\n", "- [ ] <value, key> pairs\n"], "#### Q35. To what does the Mapper map input key/value pairs?\n": ["- [ ] an average of keys for values\n", "- [ ] a sum of keys for values\n", "- [x] a set of intermediate key/value pairs\n", "- [ ] a set of final key/value pairs\n"], "#### Q36. Which Hive query returns the first 1,000 values?\n": ["- [ ] SELECT\u2026WHERE value = 1000\n", "- [x] SELECT \u2026 LIMIT 1000\n", "- [ ] SELECT TOP 1000 \u2026\n", "- [ ] SELECT MAX 1000\u2026\n"], "#### Q37. To implement high availability, how many instances of the master node should you configure?\n": ["- [ ] one\n", "- [ ] zero\n", "- [ ] shared\n", "- [x] two or more (https://data-flair.training/blogs/hadoop-high-availability-tutorial)\n"], "#### Q38. Hadoop 2.x and later implement which service as the resource coordinator?\n": ["- [ ] kubernetes\n", "- [ ] JobManager\n", "- [ ] JobTracker\n", "- [x] YARN\n"], "#### Q39. In MapReduce, **\\_** have \\_\n": ["- [ ] tasks; jobs\n", "- [ ] jobs; activities\n", "- [x] jobs; tasks\n", "- [ ] activities; tasks\n"], "#### Q40. What type of software is Hadoop Common?\n": ["- [ ] database\n", "- [x] distributed computing framework\n", "- [ ] operating system\n", "- [ ] productivity tool\n"], "#### Q41. If no reduction is desired, you should set the numbers of \\_ tasks to zero\n": ["- [ ] combiner\n", "- [x] reduce\n", "- [ ] mapper\n", "- [ ] intermediate\n"], "#### Q42. MapReduce applications use which of these classes to report their statistics?\n": ["- [ ] mapper\n", "- [ ] reducer\n", "- [ ] combiner\n", "- [x] counter\n"], "#### Q43. \\_ is the query language, and \\_ is storage for NoSQL on Hadoop\n": ["- [ ] HDFS; HQL\n", "- [x] HQL; HBase\n", "- [ ] HDFS; SQL\n", "- [ ] SQL; HBase\n"], "#### Q44. MapReduce 1.0 \\_ YARN\n": ["- [x] does not include\n", "- [ ] is the same thing as\n", "- [ ] includes\n", "- [ ] replaces\n"], "#### Q45. Which type of Hadoop node executes file system namespace operations like opening, closing, and renaming files and directories?\n": ["- [ ] ControllerNode\n", "- [ ] DataNode\n", "- [ ] MetadataNode\n", "- [x] NameNode\n"], "#### Q46. HQL queries produce which job types?\n": ["- [ ] Impala\n", "- [ ] MapReduce\n", "- [ ] Spark\n", "- [ ] Pig\n"], "#### Q47. Suppose you are trying to finish a Pig script that converts text in the input string to uppercase. What code is needed on line 2 below?\n 1 data = LOAD '/user/hue/pig/examples/data/midsummer.txt'...\n": ["- [ ] as (text:CHAR[]); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);\n", "- [x] as (text:CHARARRAY); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);\n", "- [ ] as (text:CHAR[]); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);\n", "- [ ] as (text:CHARARRAY); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);\n"], "#### Q48. In a MapReduce job, which phase runs after the Map phase completes?\n": ["- [x] Combiner\n", "- [ ] Reducer\n", "- [ ] Map2\n", "- [ ] Shuffle and Sort\n"], "#### Q49. Where would you configure the size of a block in a Hadoop environment?\n": ["- [x] dfs.block.size in hdfs-site.xmls\n", "- [ ] orc.write.variable.length.blocks in hive-default.xml\n", "- [ ] mapreduce.job.ubertask.maxbytes in mapred-site.xml\n", "- [ ] hdfs.block.size in hdfs-site.xml\n"], "#### Q50. Hadoop systems are **\\_** RDBMS systems.\n": ["- [ ] replacements for\n", "- [ ] not used with\n", "- [ ] substitutes for\n", "- [x] additions for\n"], "#### Q51. Which object can be used to distribute jars or libraries for use in MapReduce tasks?\n": ["- [x] distributed cache\n", "- [ ] library manager\n", "- [ ] lookup store\n", "- [ ] registry\n"], "#### Q52. To view the execution details of an Impala query plan, which function would you use ?\n": ["- [x] explain\n", "- [ ] query action\n", "- [ ] detail\n", "- [ ] query plan\n"], "#### Q53. Which feature is used to roll back a corrupted HDFS instance to a previously known good point in time?\n": ["- [ ] partitioning\n", "- [x] snapshot\n", "- [ ] replication\n", "- [ ] high availability\n","[Reference](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#:~:text=is%20not%20supported.-,Snapshots,known%20good%20point%20in%20time.)\n"], "#### Q54. Hadoop Common is written in which language?\n": ["- [ ] C++\n", "- [ ] C\n", "- [ ] Haskell\n", "- [x] Java\n"], "#### Q55. Which file system does Hadoop use for storage?\n": ["- [ ] NAS\n", "- [ ] FAT\n", "- [x] HDFS\n", "- [ ] NFS\n"], "#### Q56. What kind of storage and processing does Hadoop support?\n": ["- [ ] encrypted\n", "- [ ] verified\n", "- [x] distributed\n", "- [ ] remote\n"], "#### Q57. Hadoop Common consists of which components?\n": ["- [ ] Spark and YARN\n", "- [ ] HDFS and MapReduce\n", "- [ ] HDFS and S3\n", "- [ ] Spark and MapReduce\n"], "#### Q58. Most Apache Hadoop committers' work is done at which commercial company?\n": ["- [ ] Cloudera\n", "- [ ] Microsoft\n", "- [ ] Google\n", "- [ ] Amazon\n"], "#### Q59. To get information about Reducer job runs, which object should be added?\n": ["- [ ] Reporter\n", "- [ ] IntReadable\n", "- [ ] IntWritable\n", "- [ ] Writer\n"], "#### Q60. After changing the default block size and restarting the cluster, to which data does the new size apply?\n": ["- [ ] all data\n", "- [ ] no data\n", "- [ ] existing data\n", "- [ ] new data\n"], "#### Q61. Which statement should you add to improve the performance of the following query?\n": ["- [ ] GROUP BY\n", "- [ ] FILTER\n", "- [ ] SUB-SELECT\n", "- [ ] SORT\n"], "#### Q62. What custom object should you implement to reduce IO in MapReduce?\n": ["- [ ] Comparator\n", "- [ ] Mapper\n", "- [ ] Combiner\n", "- [ ] Reducer\n"], "#### Q63. You can optimize Hive queries using which method?\n": ["- [ ] secondary indices\n", "- [ ] summary statistics\n", "- [ ] column-based statistics\n", "- [ ] a primary key index\n"], "#### Q64. If you are processing a single action on each input, what type of job should you create?\n": ["- [ ] partition-only\n", "- [ ] map-only\n", "- [ ] reduce-only\n", "- [ ] combine-only\n"], "#### Q65. The simplest possible MapReduce job optimization is to perform which of these actions?\n": ["- [ ] Add more master nodes.\n", "- [ ] Implement optimized InputSplits.\n", "- [ ] Add more DataNodes.\n", "- [ ] Implement a custom Mapper.\n"], "#### Q66. When you implement a custom Writable, you must also define which of these object?\n": ["- [ ] a sort policy\n", "- [ ] a combiner policy\n", "- [ ] a compression policy\n", "- [ ] a filter policy\n"], "#### Q67. To copy a file into the Hadoop file system, what command should you use?\n": ["- [ ] hadoop fs -copy <fromDir> <toDir>\n", "- [ ] hadoop fs -copy <toDir> <fromDir>\n", "- [x] hadoop fs -copyFromLocal <fromDir> <toDir>\n", "- [ ] hadoop fs -copyFromLocal <toDir> <fromDir>"]}